# ðŸš€ Distributed DataPrime Assistant - Enterprise Architecture

This is a **complete transformation** of the DataPrime Assistant into a proper distributed system with enterprise-grade distributed tracing, showcasing single root span propagation and realistic microservices architecture.

## ðŸ—ï¸ Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Frontend  â”‚â”€â”€â”€â–¶â”‚ API Gateway  â”‚â”€â”€â”€â–¶â”‚ Query Service   â”‚â”€â”€â”€â–¶â”‚ Validation Svc  â”‚
â”‚   (Port -)  â”‚    â”‚  (Port 5000) â”‚    â”‚  (Port 5001)    â”‚    â”‚  (Port 5002)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                â”‚                        â”‚
                                                â–¼                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Database   â”‚â—€â”€â”€â”€â”‚Queue Worker  â”‚â—€â”€â”€â”€â”‚ Queue Service   â”‚â—€â”€â”€â”€â”‚ Processing Svc  â”‚
â”‚   (SQLite)  â”‚    â”‚ (Background) â”‚    â”‚  (Port 5003)    â”‚    â”‚  (Port 5004)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                                        â”‚
                                                                        â–¼
                                                                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                                                â”‚ Storage Service â”‚
                                                                â”‚  (Port 5005)    â”‚
                                                                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ðŸ”§ Services Breakdown

### ðŸŒ API Gateway (Port 5000)
- **Role**: Entry point for all requests
- **Responsibilities**: Request routing, trace context initiation, service orchestration
- **Endpoints**: `/api/generate-query`, `/api/feedback`, `/api/health`, `/api/stats`
- **Timing**: 5-10ms processing time

### ðŸ§  Query Service (Port 5001)
- **Role**: LLM-powered DataPrime query generation
- **Responsibilities**: Intent classification, OpenAI integration, query generation
- **Dependencies**: OpenAI API
- **Timing**: 800-1200ms (realistic LLM response time)

### âœ… Validation Service (Port 5002)
- **Role**: DataPrime query validation and enhancement
- **Responsibilities**: Syntax validation, performance suggestions, query optimization
- **Timing**: 50-100ms for comprehensive validation

### ðŸ“¬ Queue Service (Port 5003)
- **Role**: Message queue and background job management
- **Responsibilities**: Asynchronous processing, job queuing, background worker management
- **Features**: In-memory queue with background worker thread
- **Timing**: 10-20ms for queue operations

### âš™ï¸ Processing Service (Port 5004)
- **Role**: Background data processing and analysis
- **Responsibilities**: Query complexity analysis, performance insights, data transformation
- **Timing**: 100-200ms for complex processing

### ðŸ’¾ Storage Service (Port 5005)
- **Role**: Data persistence and feedback management
- **Responsibilities**: SQLite database operations, feedback storage, analytics tracking
- **Timing**: 20-50ms for database operations

## ðŸ”— Distributed Tracing Features

### âœ… Single Root Span Architecture
- **Root Span**: `api_gateway.generate_query_pipeline` - initiated by API Gateway
- **Child Spans**: Each service creates child spans that properly inherit trace context
- **Trace Context Propagation**: W3C TraceContext standard across all HTTP calls

### ðŸŒ³ Span Hierarchy
```
ROOT: api_gateway.generate_query_pipeline
â”œâ”€â”€ api_gateway.call_query_service
â”‚   â””â”€â”€ query_service.generate_dataprime_query
â”‚       â”œâ”€â”€ query_service.classify_intent
â”‚       â””â”€â”€ query_service.openai_generation (LLM call)
â”œâ”€â”€ api_gateway.call_validation_service
â”‚   â””â”€â”€ validation_service.validate_dataprime_query
â”‚       â”œâ”€â”€ validation_service.syntax_analysis
â”‚       â””â”€â”€ validation_service.enhancement_suggestions
â”œâ”€â”€ api_gateway.enqueue_processing
â”‚   â””â”€â”€ queue_service.enqueue_message
â”‚       â””â”€â”€ queue_service.add_to_queue
â””â”€â”€ queue_service.process_background_message
    â””â”€â”€ processing_service.process_query_data
        â”œâ”€â”€ processing_service.analyze_complexity
        â”œâ”€â”€ processing_service.generate_insights
        â”œâ”€â”€ processing_service.transform_data
        â””â”€â”€ processing_service.store_insights
            â””â”€â”€ storage_service.store_processing_data
```

### ðŸŽ¯ Service Instrumentation
- **Flask Auto-Instrumentation**: Each service automatically traces HTTP requests
- **Custom Business Logic Spans**: Manual spans for important business operations
- **OpenAI Instrumentation**: Automatic tracing of LLM calls with token usage
- **Database Instrumentation**: SQLite operations automatically traced
- **HTTP Client Instrumentation**: All inter-service calls traced

## ðŸš€ Quick Start

### 1. Prerequisites
```bash
# Ensure you have Python 3.8+ and virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install -r requirements.txt
```

### 2. Environment Setup
```bash
# Copy and configure environment variables
cp .env.example .env
# Edit .env with your API keys:
# OPENAI_API_KEY=your-openai-api-key
# CX_TOKEN=your-coralogix-token
```

### 3. Start Distributed System
```bash
# Option 1: Use the startup script (recommended)
./start_distributed_system.sh

# Option 2: Start manually
python distributed_app.py
```

### 4. Test the System
```bash
# Run comprehensive tests
python test_distributed_tracing.py

# Manual test via curl
curl -X POST http://localhost:8010/api/generate-query \
  -H "Content-Type: application/json" \
  -d '{"user_input": "Show me errors from the last hour grouped by service"}'
```

## ðŸ“Š Expected Coralogix Results

### Service Map View
- **6 interconnected services** with clear dependency relationships
- **Service topology** showing API Gateway â†’ Query/Validation â†’ Queue â†’ Processing â†’ Storage
- **Realistic latency distribution** across service boundaries

### Trace View
- **Single root trace** spanning all services
- **Total trace duration**: 1-2 seconds with proper timing breakdown:
  - API Gateway: 5-10ms
  - Query Service: 800-1200ms (LLM call dominates)
  - Validation Service: 50-100ms
  - Queue Service: 10-20ms
  - Processing Service: 100-200ms
  - Storage Service: 20-50ms

### Span Attributes
Each span includes rich attributes for observability:
- **Service identification**: `service.name`, `service.version`
- **Business context**: `user.input.query`, `query.generated`, `intent.type`
- **Performance metrics**: `processing.time_ms`, `validation.score`
- **Technical details**: `database.operation`, `queue.size`, `openai.tokens_used`

## ðŸ”§ Configuration

### Service Ports
- API Gateway: `5000`
- Query Service: `5001`
- Validation Service: `5002`
- Queue Service: `5003`
- Processing Service: `5004`
- Storage Service: `5005`

### Telemetry Configuration
```python
# Coralogix configuration
service_name="dataprime_distributed_system"
application_name="ai-dataprime-enterprise" 
subsystem_name="distributed-microservices"
```

## ðŸ› Troubleshooting

### Common Issues

1. **Services won't start**
   ```bash
   # Check if ports are already in use
   lsof -i :5000-5005
   
   # Kill existing processes
   ./cleanup_port.sh
   ```

2. **Missing dependencies**
   ```bash
   pip install opentelemetry-instrumentation-requests>=0.48b0
   ```

3. **Environment variables not set**
   ```bash
   # Check your .env file
   cat .env
   # Should contain OPENAI_API_KEY and CX_TOKEN
   ```

4. **Services can't communicate**
   ```bash
   # Test individual service health
   curl http://localhost:8011/health
   curl http://localhost:8012/health
   # etc.
   ```

### Health Checks
Each service provides a `/health` endpoint:
- `GET http://localhost:8010/api/health` - API Gateway + downstream health
- `GET http://localhost:8011/health` - Query Service
- `GET http://localhost:8012/health` - Validation Service
- `GET http://localhost:8013/health` - Queue Service
- `GET http://localhost:8014/health` - Processing Service
- `GET http://localhost:8015/health` - Storage Service

## ðŸ“ˆ Monitoring & Observability

### Key Metrics to Monitor
1. **Request Success Rate**: API Gateway success/error ratio
2. **Service Latency**: P95 response times per service
3. **Queue Depth**: Background processing queue size
4. **Database Performance**: Storage service operation timing
5. **OpenAI Usage**: Token consumption and response times

### Coralogix Dashboards
Look for these patterns in your Coralogix dashboard:
- **Service Dependencies**: Clear service-to-service call patterns
- **Error Propagation**: How errors flow through the system
- **Performance Bottlenecks**: Which services contribute most to latency
- **Business Metrics**: Query success rates, user satisfaction scores

## ðŸŽ¯ Demo Script

For demonstrations, use this flow:

1. **Start System**: `./start_distributed_system.sh`
2. **Generate Query**: Submit a complex query via API Gateway
3. **Show Coralogix**: Display the resulting trace with single root span
4. **Highlight Architecture**: Point out 6 connected services
5. **Discuss Timing**: Show realistic enterprise timing distribution
6. **Submit Feedback**: Demonstrate feedback storage flow
7. **Show Service Map**: Display service dependency graph

## ðŸ”® Future Enhancements

- **Redis Integration**: Replace in-memory queue with Redis
- **Kubernetes Deployment**: Container orchestration configuration
- **Circuit Breakers**: Resilience patterns for service failures
- **Rate Limiting**: API Gateway rate limiting and throttling
- **Metrics Collection**: Prometheus metrics export
- **Service Mesh**: Istio integration for advanced observability

---

**This distributed architecture demonstrates enterprise-grade observability with proper trace context propagation, realistic service complexity, and comprehensive monitoring capabilities.**
