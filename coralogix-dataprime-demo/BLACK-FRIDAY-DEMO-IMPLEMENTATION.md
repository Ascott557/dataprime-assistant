# Black Friday Demo Implementation - Complete

## üéØ Overview

A production-ready Black Friday failure simulation demo that demonstrates:
- **Database Performance Degradation**: Missing indexes causing 2500ms+ queries
- **Connection Pool Exhaustion**: 35% failure rate at peak load
- **Realistic Traffic Patterns**: Progressive load from 30‚Üí120 requests/minute
- **Full Observability**: OpenTelemetry instrumentation with Flow Alert triggering

## ‚úÖ Implementation Status

All 6 critical issues from the requirements have been addressed:

### 1. Unix Timestamp Synchronization ‚úÖ
- **Problem**: String time parsing caused drift across services
- **Solution**: `DEMO_START_TIMESTAMP` environment variable (Unix epoch)
- **Implementation**: `shared_span_attributes.py` with `calculate_demo_minute()`

### 2. Consistent Span Attributes ‚úÖ
- **Problem**: Inconsistent attribute naming broke Flow Alerts
- **Solution**: Shared `DemoSpanAttributes` class
- **Critical Attributes**:
  - `db.slow_query`, `db.full_table_scan`, `db.missing_index`
  - `db.connection.pool.exhausted` (consistent naming)
  - `checkout.failed`, `checkout.failure_reason`

### 3. Thread-Safe Load Generator ‚úÖ
- **Problem**: Race conditions in state management
- **Solution**: `demo_state` dict with `Lock()` for thread safety
- **Features**: Non-daemon threads, clean shutdown, state validation

### 4. Proper Kubernetes Operations ‚úÖ
- **Problem**: Direct SQL execution failed
- **Solution**: `kubectl cp` to copy SQL file, then execute
- **Verification**: Confirms indexes dropped with `\di products*`

### 5. ConfigMap Update Verification ‚úÖ
- **Problem**: Pods didn't pick up new config
- **Solution**: `kubectl wait`, explicit verification, fail-fast checks
- **Process**: Restart ‚Üí Wait ‚Üí Verify ‚Üí Fail if incorrect

### 6. Structured Logging ‚úÖ
- **Problem**: No structured data for investigation
- **Solution**: `structlog` with JSON renderer throughout
- **Benefits**: Queryable logs, trace correlation, investigation evidence

## üìÅ Files Created/Modified

### New Files

1. **`app/shared_span_attributes.py`**
   - `DemoSpanAttributes` class with consistent Flow Alert attributes
   - `calculate_demo_minute()` using Unix timestamps
   - `is_demo_mode()` and `get_demo_phase()` helpers

2. **`infrastructure/database/missing_index_config.sql`**
   - Drops `idx_products_category` and `idx_products_stock_quantity`
   - Fully documented scenario and expected impact
   - Commented restoration commands

3. **`infrastructure/database/connection_pool_config.yaml`**
   - Documents 3 critical misconfigurations
   - max_connections: 20 (too low)
   - connection_timeout: 5s (too aggressive)
   - read_replicas.enabled: false (root cause)

4. **`infrastructure/database/setup_demo_database_issues.sh`**
   - Pre-flight checks (kubectl, namespace, PostgreSQL)
   - `kubectl cp` for SQL file
   - Index verification with `\di products*`
   - ConfigMap updates
   - Idempotent design

5. **`services/black_friday_scenario.py`**
   - `BlackFridayScenario` class
   - Progressive traffic (30‚Üí120 RPM)
   - User journey definitions (browse_only, browse_and_cart, full_checkout)
   - Journey weight adjustments during failures
   - Complete statistics tracking

6. **`scripts/run-demo.sh`**
   - Comprehensive orchestration script
   - Pre-flight checks (kubectl, services, PostgreSQL, OTel)
   - Database configuration
   - Unix timestamp creation and ConfigMap updates
   - Service restart with verification
   - Real-time monitoring (30 minutes)
   - Automatic cleanup on exit

### Modified Files

1. **`services/product_catalog_service.py`**
   - Imported `shared_span_attributes`
   - Added `get_progressive_delay()` function
   - Updated 3 endpoints to use `DemoSpanAttributes.set_slow_query()`
   - Structured logging with `structlog`

2. **`services/checkout_service.py`**
   - Added `ConnectionPoolSimulator` class
   - Progressive failure rates (0% ‚Üí 35%)
   - `should_fail_connection()` with shared attributes
   - Updated 2 endpoints to check pool exhaustion
   - Structured logging

3. **`services/load_generator.py`**
   - Added thread-safe `demo_state` management
   - New endpoint: `POST /admin/start-demo`
   - New endpoint: `GET /admin/demo-status`
   - New endpoint: `POST /admin/stop-demo`
   - Non-daemon thread for clean shutdown

4. **`requirements.txt` and `docker/requirements-minimal.txt`**
   - Already had `structlog==24.2.0` ‚úÖ

## üöÄ Usage

### Quick Start

```bash
# Navigate to project root
cd /Users/andrescott/dataprime-assistant-1/coralogix-dataprime-demo

# Run the demo
./scripts/run-demo.sh
```

### What Happens

1. **Pre-flight checks** (5 seconds)
   - Verifies kubectl, namespace, services, PostgreSQL, OTel Collector

2. **Database configuration** (10 seconds)
   - Drops indexes
   - Sets connection pool to 20
   - Verifies changes

3. **Service restart** (30 seconds)
   - Updates ConfigMap with `DEMO_MODE=blackfriday` and Unix timestamp
   - Restarts all deployments
   - Waits for rollout
   - Verifies pods have correct config

4. **Traffic generation** (30 minutes)
   - Minute 0-10: Ramp from 30‚Üí120 RPM
   - Minute 10-15: Peak traffic, normal operations
   - Minute 15-20: Database degradation (2500ms queries)
   - Minute 20-25: Connection pool exhaustion begins
   - **Minute 22-23: üö® FLOW ALERT TRIGGERS**
   - Minute 25-30: Peak failure rate (35%)

5. **Automatic cleanup** (on exit)
   - Restores indexes
   - Resets ConfigMap
   - Stops traffic generation

### Monitoring

The script provides real-time output:

```
--- Minute 18/30 ---
  Phase: degradation | RPM: 120 | Requests: 2160 | Errors: 108 (5.0%)

--- Minute 22/30 ---
  Phase: critical | RPM: 120 | Requests: 2640 | Errors: 740 (28.0%)
  üö®üö®üö® FLOW ALERT SHOULD TRIGGER NOW üö®üö®üö®
  üìß Check Coralogix ‚Üí Incidents
  üîó https://eu2.coralogix.com/incidents
```

### Manual Testing (Alternative)

If you want to test components individually:

```bash
# 1. Apply database configuration only
./infrastructure/database/setup_demo_database_issues.sh

# 2. Start traffic via kubectl
LOAD_POD=$(kubectl get pods -n ecommerce-demo -l app=load-generator -o jsonpath='{.items[0].metadata.name}')

kubectl exec -n ecommerce-demo $LOAD_POD -- \
  curl -X POST http://localhost:8010/admin/start-demo \
  -H 'Content-Type: application/json' \
  -d '{"scenario": "blackfriday", "duration_minutes": 30, "peak_rpm": 120}'

# 3. Check status
kubectl exec -n ecommerce-demo $LOAD_POD -- \
  curl http://localhost:8010/admin/demo-status | jq

# 4. Stop demo
kubectl exec -n ecommerce-demo $LOAD_POD -- \
  curl -X POST http://localhost:8010/admin/stop-demo
```

## üîç Verification Checklist

### Before Running Demo

- [ ] All services are Running in `ecommerce-demo` namespace
- [ ] PostgreSQL is accepting connections
- [ ] Coralogix OTel Collector is deployed
- [ ] `CX_TOKEN` and `CX_ENDPOINT` are configured

### During Demo

- [ ] Traffic ramps from 30‚Üí120 RPM (minutes 0-10)
- [ ] Product queries slow to 2500ms+ (minute 15+)
- [ ] Checkout errors appear (minute 20+)
- [ ] Error rate reaches ~35% (minute 25+)

### After Demo

- [ ] Indexes are restored (`\di products*` shows both indexes)
- [ ] ConfigMap has `DEMO_MODE=off` and `DB_MAX_CONNECTIONS=100`
- [ ] Services return to normal operation

## üéØ Flow Alert Configuration

To trigger the Flow Alert, configure it with these conditions:

### Trigger Conditions

```yaml
conditions:
  - attribute: "db.slow_query"
    operator: "equals"
    value: true
  - attribute: "db.full_table_scan"
    operator: "equals"
    value: true
  - attribute: "db.connection.pool.exhausted"
    operator: "equals"
    value: true
  - attribute: "checkout.failed"
    operator: "equals"
    value: true
    
threshold:
  - error_rate > 0.20  # 20% errors
  - duration_p95 > 2000  # 2000ms P95
```

### Expected Timeline

- **Minute 15**: First slow query alerts
- **Minute 20**: Connection pool alerts start
- **Minute 22**: Threshold breached ‚Üí **FLOW ALERT TRIGGERS**
- **Minute 25**: Peak failure state

## üìä Success Metrics

After running the demo, you should see:

1. **Coralogix Traces**:
   - Application: `ecommerce-platform`
   - Services: `product-catalog`, `checkout`, `cart`, `load-generator`
   - Spans with all required attributes

2. **Coralogix Logs**:
   - Structured JSON logs with demo events
   - `demo_slow_query_triggered` events
   - `demo_pool_exhaustion` events
   - `demo_checkout_failed` events

3. **Coralogix Incidents**:
   - Flow Alert triggered at minute ~22
   - Root cause: Database misconfiguration
   - Revenue impact: ~$50K/minute in failed checkouts

## üîß Troubleshooting

### Demo won't start

```bash
# Check load generator pod
kubectl get pods -n ecommerce-demo -l app=load-generator

# Check load generator logs
kubectl logs -n ecommerce-demo -l app=load-generator --tail=50

# Verify black_friday_scenario.py exists
kubectl exec -n ecommerce-demo $LOAD_POD -- ls -la /app/services/black_friday_scenario.py
```

### Indexes not dropping

```bash
# Check PostgreSQL connection
kubectl exec -n ecommerce-demo postgresql-primary-0 -- pg_isready -U ecommerce_user

# Manually verify indexes
kubectl exec -n ecommerce-demo postgresql-primary-0 -- \
  psql -U ecommerce_user -d ecommerce -c "\di products*"

# Manually drop if needed
kubectl exec -n ecommerce-demo postgresql-primary-0 -- \
  psql -U ecommerce_user -d ecommerce -c "DROP INDEX IF EXISTS idx_products_category;"
```

### ConfigMap not updating

```bash
# Check current ConfigMap
kubectl get configmap ecommerce-config -n ecommerce-demo -o yaml

# Manually patch
kubectl patch configmap ecommerce-config -n ecommerce-demo \
  --patch '{"data": {"DEMO_MODE": "blackfriday", "DEMO_START_TIMESTAMP": "'"$(date +%s)"'"}}'

# Restart deployments
kubectl rollout restart deployment -n ecommerce-demo
```

## üéì Learning Outcomes

After running this demo, attendees will understand:

1. **How database misconfigurations cause failures**
   - Missing indexes ‚Üí full table scans
   - Connection pool sizing for peak load
   - Read replica distribution

2. **How to investigate with OpenTelemetry**
   - Span attributes reveal root cause
   - Trace propagation across services
   - Correlation between logs and traces

3. **How Flow Alerts detect complex issues**
   - Multiple conditions (slow queries + pool exhaustion)
   - Business impact tracking (failed checkouts)
   - Automatic incident creation

## üìù Notes

- Demo uses **Unix timestamps** for reliable synchronization
- All span attributes use **consistent naming** for Flow Alerts
- Load generator uses **thread-safe state** management
- Kubernetes operations use **proper kubectl cp** patterns
- ConfigMap updates include **verification** steps
- Structured logging provides **investigation evidence**

## üöÄ Next Steps

1. **Run the demo**: `./scripts/run-demo.sh`
2. **Monitor Coralogix**: Watch for Flow Alert at minute ~22
3. **Investigate**: Use Coralogix to trace back to root cause
4. **Present**: Show how observability saves Black Friday

---

**Implementation Complete**: All 6 critical issues addressed ‚úÖ
**Ready for Production**: Tested patterns, safety checks, cleanup ‚úÖ
**Documentation**: Comprehensive usage and troubleshooting ‚úÖ

